{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-16T08:11:57.307769Z",
     "start_time": "2023-08-16T08:11:57.293210Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "\n",
    "import pandas as pd\n",
    "from avalanche.benchmarks import benchmark_with_validation_stream, nc_benchmark\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from avalanche.training.supervised import EWC, icarl, Naive, CWRStar, Replay, GDumb, LwF, GEM, AGEM, EWC\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, loss_metrics, timing_metrics, \\\n",
    "    cpu_usage_metrics, disk_usage_metrics\n",
    "from avalanche.training.plugins import EWCPlugin, AGEMPlugin, GEMPlugin, ReplayPlugin, CWRStarPlugin, RWalkPlugin\n",
    "from avalanche.logging import InteractiveLogger, TensorboardLogger, TextLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training import ExemplarsBuffer, ReservoirSamplingBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [
    "\n",
    "class TorchDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filePath):\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Read CSV\n",
    "        data = pd.read_csv(filePath)\n",
    "        # drop one class from dataset to make it 30 classes\n",
    "        data = data[data['label'] != 30]\n",
    "\n",
    "        self.X = data.iloc[:, :-1].values\n",
    "        self.targets = data.iloc[:, -1].values\n",
    "\n",
    "        # Feature Scale if you want\n",
    "\n",
    "        # Convert to Torch Tensors\n",
    "        self.X = torch.tensor(self.X, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(self.targets, dtype=torch.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.X[item], self.targets[item]\n",
    "\n",
    "\n",
    "def prep_benchmark(train_loc, test_loc):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(device)\n",
    "\n",
    "    hdata_train = TorchDataset(train_loc)\n",
    "    hdata_test = TorchDataset(test_loc)\n",
    "\n",
    "    return benchmark_with_validation_stream(nc_benchmark(train_dataset=hdata_train, test_dataset=hdata_test\n",
    "                                                         , shuffle=True, seed=1234, task_labels=True, n_experiences=5,\n",
    "                                                         one_dataset_per_exp=True,\n",
    "\n",
    "                                                         ))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T08:11:57.310965Z",
     "start_time": "2023-08-16T08:11:57.302379Z"
    }
   },
   "id": "6b61e45f9d765b6"
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [],
   "source": [
    "from avalanche.benchmarks import CLExperience\n",
    "from avalanche.models import DynamicModule\n",
    "\n",
    "\n",
    "# implement a Incremental classifier with a custom classifier\n",
    "class IncrementalClassifierD1(DynamicModule):\n",
    "    \"\"\"\n",
    "    Output layer that incrementally adds units whenever new classes are\n",
    "    encountered.\n",
    "\n",
    "    Typically used in class-incremental benchmarks where the number of\n",
    "    classes grows over time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        initial_out_features=2,\n",
    "        masking=True,\n",
    "        mask_value=-1000,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param in_features: number of input features.\n",
    "        :param initial_out_features: initial number of classes (can be\n",
    "            dynamically expanded).\n",
    "        :param masking: whether unused units should be masked (default=True).\n",
    "        :param mask_value: the value used for masked units (default=-1000).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.masking = masking\n",
    "        self.mask_value = mask_value\n",
    "        # self.features = nn.Sequential(\n",
    "        # nn.Conv1d(in_channels=1, out_channels=32, padding='same',  kernel_size=3,),\n",
    "        # nn.MaxPool1d(4),\n",
    "        # nn.Conv1d(in_channels=32, out_channels=32, padding='same',  kernel_size=3),\n",
    "        # nn.MaxPool1d(4),\n",
    "        # nn.Conv1d(in_channels=32, out_channels=16, padding='same',  kernel_size=3),\n",
    "        # nn.MaxPool1d(4),\n",
    "        # nn.Conv1d(in_channels=16, out_channels=16, padding='same',  kernel_size=3),\n",
    "        # nn.MaxPool1d(4),\n",
    "        # )\n",
    "        # \n",
    "        # self.fc1 = nn.Linear(256, 300)\n",
    "        # self.fc2 = nn.Linear(300, 128)\n",
    "        # self.fc3 = nn.Linear(128, 31)\n",
    "        self.conv1D_1 = nn.Conv1d(in_channels=1, out_channels=32, padding='same',  kernel_size=3,)\n",
    "        self.maxPool1D_1 = nn.MaxPool1d(4)\n",
    "        self.conv1D_2 = nn.Conv1d(in_channels=32, out_channels=32, padding='same',  kernel_size=3)\n",
    "        self.maxPool1D_2 = nn.MaxPool1d(4)\n",
    "        self.conv1D_3 = nn.Conv1d(in_channels=32, out_channels=16, padding='same',  kernel_size=3)\n",
    "        self.maxPool1D_3 = nn.MaxPool1d(4)\n",
    "        self.conv1D_4 = nn.Conv1d(in_channels=16, out_channels=16, padding='same',  kernel_size=3)\n",
    "        self.maxPool1D_4 = nn.MaxPool1d(4)\n",
    "\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(256, 300)\n",
    "        self.fc2 = nn.Linear(300, 128)\n",
    "        self.fc3 = nn.Linear(128, 31)\n",
    "        self.classifier = nn.Linear(256, initial_out_features)\n",
    "        # self.initial_out_features = initial_out_features\n",
    "\n",
    "        # self.classifier = torch.nn.Linear(in_features, initial_out_features)\n",
    "        au_init = torch.zeros(initial_out_features, dtype=torch.bool)\n",
    "        self.register_buffer(\"active_units\", au_init)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def adaptation(self, experience: CLExperience):\n",
    "        \"\"\"If `dataset` contains unseen classes the classifier is expanded.\n",
    "\n",
    "        :param experience: data from the current experience.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        in_features = self.classifier.in_features\n",
    "        old_nclasses = self.classifier.out_features\n",
    "        curr_classes = experience.classes_in_this_experience\n",
    "        new_nclasses = max(self.classifier.out_features, max(curr_classes) + 1)\n",
    "\n",
    "        # update active_units mask\n",
    "        if self.masking:\n",
    "            if old_nclasses != new_nclasses:  # expand active_units mask\n",
    "                old_act_units = self.active_units\n",
    "                self.active_units = torch.zeros(new_nclasses, dtype=torch.bool)\n",
    "                self.active_units[: old_act_units.shape[0]] = old_act_units\n",
    "            # update with new active classes\n",
    "            if self.training:\n",
    "                self.active_units[curr_classes] = 1\n",
    "\n",
    "        # update classifier weights\n",
    "        if old_nclasses == new_nclasses:\n",
    "            return\n",
    "        old_w, old_b = self.classifier.weight, self.classifier.bias\n",
    "        self.classifier = torch.nn.Linear(in_features, new_nclasses)\n",
    "        self.classifier.weight[:old_nclasses] = old_w\n",
    "        self.classifier.bias[:old_nclasses] = old_b\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        \"\"\"compute the output given the input `x`. This module does not use\n",
    "        the task label.\n",
    "\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        # block 1\n",
    "        x = x.unsqueeze(dim=1)\n",
    "        x = self.conv1D_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x= self.maxPool1D_1(x)\n",
    "\n",
    "\n",
    "        # block 2\n",
    "        x = self.conv1D_2(x)\n",
    "        x = torch.relu(x)\n",
    "        x= self.maxPool1D_2(x)\n",
    "\n",
    "        # block 3\n",
    "        x = self.conv1D_3(x)\n",
    "        x = torch.relu(x)\n",
    "        x= self.maxPool1D_3(x)\n",
    "\n",
    "        # block 4\n",
    "        x = self.conv1D_4(x)\n",
    "        x = torch.relu(x)\n",
    "        x= self.maxPool1D_4(x)\n",
    "\n",
    "        # Flatten\n",
    "\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        out = torch.log_softmax(x, dim=1)\n",
    "       \n",
    "        if self.masking:\n",
    "            print(out[..., torch.logical_not(self.active_units)])\n",
    "            print(out[..., torch.logical_not(self.active_units)].shape)\n",
    "            out[..., torch.logical_not(self.active_units)] = self.mask_value\n",
    "        return out\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T08:11:57.321388Z",
     "start_time": "2023-08-16T08:11:57.318259Z"
    }
   },
   "id": "b59ca7a8c756557a"
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 16])\n",
      "torch.Size([1, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "features = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=1, out_channels=32, padding='same',  kernel_size=3,),\n",
    "        nn.MaxPool1d(4),\n",
    "        nn.Conv1d(in_channels=32, out_channels=32, padding='same',  kernel_size=3),\n",
    "        nn.MaxPool1d(4),\n",
    "        nn.Conv1d(in_channels=32, out_channels=16, padding='same',  kernel_size=3),\n",
    "        nn.MaxPool1d(4),\n",
    "        nn.Conv1d(in_channels=16, out_channels=16, padding='same',  kernel_size=3),\n",
    "        nn.MaxPool1d(4),\n",
    "        )\n",
    "sample_input = torch.randn(1, 1, 4096)\n",
    "\n",
    "sample_output = features(sample_input)\n",
    "print(sample_output.shape)\n",
    "smaple_output = sample_output.view(sample_output.size(0), -1)\n",
    "        \n",
    "print(sample_output.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T08:11:57.336442Z",
     "start_time": "2023-08-16T08:11:57.324923Z"
    }
   },
   "id": "cbbb9daec2b4a2be"
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [],
   "source": [
    "from avalanche.models import IncrementalClassifier\n",
    "\n",
    "model4 = IncrementalClassifierD1(in_features=4096, masking=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T08:11:57.345134Z",
     "start_time": "2023-08-16T08:11:57.337494Z"
    }
   },
   "id": "bf7b91c2f6281acd"
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "  0%|          | 0/1 [26:19<?, ?it/s]\n",
      "  0%|          | 0/1 [24:55<?, ?it/s]\n",
      "  0%|          | 0/1 [24:06<?, ?it/s]\n",
      "  0%|          | 0/1 [17:42<?, ?it/s]\n",
      "  0%|          | 0/1 [04:38<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "benchmark = prep_benchmark(train_loc='./DATA/TRAIN_DATA.csv', test_loc='./DATA/TEST_DATA.csv')\n",
    "\n",
    "# log to Tensorboard\n",
    "tb_logger = TensorboardLogger()\n",
    "\n",
    "# log to text file\n",
    "text_logger = TextLogger(open('log.txt', 'a'))\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    # confusion_matrix_metrics(num_classes=benchmark['inc_bench'].n_classes, save_image=False,\n",
    "    #                          stream=True),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger, tb_logger]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T08:11:58.612348Z",
     "start_time": "2023-08-16T08:11:57.345557Z"
    }
   },
   "id": "9ff077cc9b04cf06"
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [2, 14, 15]\n",
      "-- >> Start of training phase << --\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [16] at index 0 does not match the shape of the indexed tensor [57, 31] at index 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[213], line 23\u001B[0m\n\u001B[1;32m     21\u001B[0m classes_exp\u001B[38;5;241m.\u001B[39mappend(experience\u001B[38;5;241m.\u001B[39mclasses_in_this_experience)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# train returns a dictionary which contains all the metric values\u001B[39;00m\n\u001B[0;32m---> 23\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mcl_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexperience\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining completed\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mComputing accuracy on the whole test set\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Pytorch39/lib/python3.9/site-packages/avalanche/training/templates/base_sgd.py:146\u001B[0m, in \u001B[0;36mBaseSGDTemplate.train\u001B[0;34m(self, experiences, eval_streams, **kwargs)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    140\u001B[0m           experiences: Union[CLExperience,\n\u001B[1;32m    141\u001B[0m                              ExpSequence],\n\u001B[1;32m    142\u001B[0m           eval_streams: Optional[Sequence[Union[CLExperience,\n\u001B[1;32m    143\u001B[0m                                                 ExpSequence]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    144\u001B[0m           \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 146\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexperiences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_streams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluator\u001B[38;5;241m.\u001B[39mget_last_metrics()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Pytorch39/lib/python3.9/site-packages/avalanche/training/templates/base.py:116\u001B[0m, in \u001B[0;36mBaseTemplate.train\u001B[0;34m(self, experiences, eval_streams, **kwargs)\u001B[0m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperience \u001B[38;5;129;01min\u001B[39;00m experiences:\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_before_training_exp(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_exp\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexperience\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_streams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_training_exp(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_training(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Pytorch39/lib/python3.9/site-packages/avalanche/training/templates/base_sgd.py:264\u001B[0m, in \u001B[0;36mBaseSGDTemplate._train_exp\u001B[0;34m(self, experience, eval_streams, **kwargs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stop_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m--> 264\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_training_epoch(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Pytorch39/lib/python3.9/site-packages/avalanche/training/templates/update_type/sgd_update.py:21\u001B[0m, in \u001B[0;36mSGDUpdate.training_epoch\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Forward\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_before_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmb_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Loss & Backward\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Pytorch39/lib/python3.9/site-packages/avalanche/training/templates/problem_type/supervised_problem.py:27\u001B[0m, in \u001B[0;36mSupervisedProblem.forward\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;124;03m\"\"\"Compute the model's output given the current mini-batch.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mavalanche_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmb_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmb_task_id\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Pytorch39/lib/python3.9/site-packages/avalanche/models/utils.py:13\u001B[0m, in \u001B[0;36mavalanche_forward\u001B[0;34m(model, x, task_labels)\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model(x, task_labels)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# no task labels\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Pytorch39/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[209], line 139\u001B[0m, in \u001B[0;36mIncrementalClassifierD1.forward\u001B[0;34m(self, x, **kwargs)\u001B[0m\n\u001B[1;32m    136\u001B[0m out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlog_softmax(x, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmasking:\n\u001B[0;32m--> 139\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[43mout\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogical_not\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactive_units\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28mprint\u001B[39m(out[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, torch\u001B[38;5;241m.\u001B[39mlogical_not(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactive_units)]\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m    141\u001B[0m     out[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, torch\u001B[38;5;241m.\u001B[39mlogical_not(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactive_units)] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmask_value\n",
      "\u001B[0;31mIndexError\u001B[0m: The shape of the mask [16] at index 0 does not match the shape of the indexed tensor [57, 31] at index 1"
     ]
    }
   ],
   "source": [
    "cl_strategy = EWC(\n",
    "    model=model4,\n",
    "    optimizer=torch.optim.Adam(model4.parameters(), lr=0.001, ),\n",
    "    criterion=CrossEntropyLoss(),\n",
    "    train_mb_size=500, train_epochs=20, eval_mb_size=100,\n",
    "    ewc_lambda=0.4,\n",
    "    evaluator=eval_plugin,\n",
    "    plugins=[ReplayPlugin(mem_size=10000, storage_policy=ReservoirSamplingBuffer(max_size=10000)),\n",
    "             ]\n",
    ")\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "model_incs = []\n",
    "classes_exp = []\n",
    "for experience in benchmark.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "    # model_incs.append(model4.classifier.out_features)\n",
    "    classes_exp.append(experience.classes_in_this_experience)\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(benchmark.test_stream))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-16T08:11:58.747832Z",
     "start_time": "2023-08-16T08:11:58.615235Z"
    }
   },
   "id": "69ad8d0996a04f2b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
