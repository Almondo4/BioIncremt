{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-06T16:13:46.055126Z",
     "start_time": "2023-06-06T16:13:46.052661Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics import Accuracy, F1Score, Precision, AUROC\n",
    "import pytorch_lightning as pl\n",
    "import torchsummary\n",
    "\n",
    "from torchmetrics import Accuracy, F1Score, Precision, AUROC, ConfusionMatrix\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import pandas as pd\n",
    "from avalanche.benchmarks import benchmark_with_validation_stream, nc_benchmark\n",
    "from torch.utils.data import Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T16:13:50.484816Z",
     "start_time": "2023-06-06T16:13:46.053642Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class TorchDataset(Dataset):\n",
    "\n",
    "    def __init__(self,filePath):\n",
    "\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Read CSV\n",
    "        data = pd.read_csv(filePath)\n",
    "        data = data[data['label'] != 30]\n",
    "        #TODO: remove the last class\n",
    "        self.X = data.iloc[:,:-1].values\n",
    "        self.targets = data.iloc[:, -1].values\n",
    "\n",
    "        # Feature Scale if you want\n",
    "\n",
    "\n",
    "        # Convert to Torch Tensors\n",
    "        self.X = torch.tensor(self.X, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(self.targets, dtype=torch.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        return self.X[item], self.targets[item]\n",
    "\n",
    "\n",
    "def prep_benchmark(train_loc, test_loc):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(device)\n",
    "\n",
    "    hdata_train = TorchDataset(train_loc)\n",
    "    hdata_test = TorchDataset(test_loc)\n",
    "\n",
    "    return benchmark_with_validation_stream(nc_benchmark(train_dataset=hdata_train, test_dataset=hdata_test\n",
    "                                 , shuffle=True, seed=1234, task_labels=True, n_experiences=5,\n",
    "                                 ))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T16:13:50.489939Z",
     "start_time": "2023-06-06T16:13:50.488075Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from avalanche.models import DynamicModule\n",
    "from avalanche.benchmarks import CLExperience\n",
    "\n",
    "\n",
    "class Incremental1DClassifier(DynamicModule):\n",
    "    \"\"\"\n",
    "    Output layer that incrementally adds units whenever new classes are\n",
    "    encountered.\n",
    "\n",
    "    Typically used in class-incremental benchmarks where the number of\n",
    "    classes grows over time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        initial_out_features=2,\n",
    "        masking=True,\n",
    "        mask_value=-1000,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param in_features: number of input features.\n",
    "        :param initial_out_features: initial number of classes (can be\n",
    "            dynamically expanded).\n",
    "        :param masking: whether unused units should be masked (default=True).\n",
    "        :param mask_value: the value used for masked units (default=-1000).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.masking = masking\n",
    "        self.mask_value = mask_value\n",
    "\n",
    "        self.classifier = torch.nn.Linear(in_features, initial_out_features)\n",
    "        au_init = torch.zeros(initial_out_features, dtype=torch.bool)\n",
    "        self.register_buffer(\"active_units\", au_init)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def adaptation(self, experience: CLExperience):\n",
    "        \"\"\"If `dataset` contains unseen classes the classifier is expanded.\n",
    "\n",
    "        :param experience: data from the current experience.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        in_features = self.classifier.in_features\n",
    "        old_nclasses = self.classifier.out_features\n",
    "        curr_classes = experience.classes_in_this_experience\n",
    "        new_nclasses = max(self.classifier.out_features, max(curr_classes) + 1)\n",
    "\n",
    "        # update active_units mask\n",
    "        if self.masking:\n",
    "            if old_nclasses != new_nclasses:  # expand active_units mask\n",
    "                old_act_units = self.active_units\n",
    "                self.active_units = torch.zeros(new_nclasses, dtype=torch.bool)\n",
    "                self.active_units[: old_act_units.shape[0]] = old_act_units\n",
    "            # update with new active classes\n",
    "            if self.training:\n",
    "                self.active_units[curr_classes] = 1\n",
    "\n",
    "        # update classifier weights\n",
    "        if old_nclasses == new_nclasses:\n",
    "            return\n",
    "        old_w, old_b = self.classifier.weight, self.classifier.bias\n",
    "        self.classifier = torch.nn.Linear(in_features, new_nclasses)\n",
    "        self.classifier.weight[:old_nclasses] = old_w\n",
    "        self.classifier.bias[:old_nclasses] = old_b\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        \"\"\"compute the output given the input `x`. This module does not use\n",
    "        the task label.\n",
    "\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        out = self.classifier(x)\n",
    "        if self.masking:\n",
    "            out[..., torch.logical_not(self.active_units)] = self.mask_value\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T16:13:50.497744Z",
     "start_time": "2023-06-06T16:13:50.490173Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from avalanche.models.dynamic_modules import (\n",
    "    MultiTaskModule,\n",
    "    MultiHeadClassifier, IncrementalClassifier,\n",
    ")\n",
    "\n",
    "\n",
    "class Simple1DCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network\n",
    "\n",
    "    **Example**::\n",
    "\n",
    "         from avalanche.models import SimpleCNN\n",
    "         n_classes = 10 # e.g. MNIST\n",
    "         model = SimpleCNN(num_classes=n_classes)\n",
    "         print(model) # View model details\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Simple1DCNN, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "\n",
    "        nn.Conv1d(in_channels=1, out_channels=32, padding='same',  kernel_size=3,),\n",
    "        nn.MaxPool1d(4),\n",
    "        nn.Conv1d(in_channels=32, out_channels=32, padding='same',  kernel_size=3),\n",
    "        nn.MaxPool1d(4),\n",
    "        nn.Conv1d(in_channels=32, out_channels=16, padding='same',  kernel_size=3),\n",
    "        nn.MaxPool1d(4),\n",
    "        nn.Conv1d(in_channels=16, out_channels=16, padding='same',  kernel_size=3),\n",
    "        nn.MaxPool1d(4),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(nn.Linear(256, num_classes))\n",
    "\n",
    "\n",
    "# IN MTS it doesn't even use this foward\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(dim=1)\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # x = self.classifier(x, task_labels)#TODO: previously\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class MTSimple1DCNN(Simple1DCNN, MultiTaskModule):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network\n",
    "    with multi-head classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classifier = MultiHeadClassifier(in_features=256,initial_out_features=6)\n",
    "\n",
    "# CHECK: `initial_out_features`. is the problem her it has to be consistant, so we are not going to use a multitask module\n",
    "#     def forward(self, x, task_labels):\n",
    "#         x = x.unsqueeze(dim=1)\n",
    "#         x = self.features(x)\n",
    "#         x = x.squeeze(dim=1)\n",
    "#         x = self.classifier(x,task_labels)\n",
    "#         return x\n",
    "\n",
    "\n",
    "    def forward(self, x, task_labels):\n",
    "        print(f'before : {x.shape}')\n",
    "        x = x.unsqueeze(dim=1)\n",
    "        x = self.features(x)\n",
    "        print(f'after : {x.shape}')\n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(f'after squeuze : {x.shape}')\n",
    "        x = self.classifier(x, task_labels)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T16:13:50.509149Z",
     "start_time": "2023-06-06T16:13:50.504115Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "class CNN1D(pl.LightningModule):\n",
    "    def __init__(self, input_shape):\n",
    "\n",
    "        self.save_hyperparameters()# to Save Hyperparameters\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1D_1 = nn.Conv1d(in_channels=1, out_channels=32, padding='same',  kernel_size=3,)\n",
    "        self.maxPool1D_1 = nn.MaxPool1d(4)\n",
    "        self.conv1D_2 = nn.Conv1d(in_channels=32, out_channels=32, padding='same',  kernel_size=3)\n",
    "        self.maxPool1D_2 = nn.MaxPool1d(4)\n",
    "        self.conv1D_3 = nn.Conv1d(in_channels=32, out_channels=16, padding='same',  kernel_size=3)\n",
    "        self.maxPool1D_3 = nn.MaxPool1d(4)\n",
    "        self.conv1D_4 = nn.Conv1d(in_channels=16, out_channels=16, padding='same',  kernel_size=3)\n",
    "        self.maxPool1D_4 = nn.MaxPool1d(4)\n",
    "\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(256, 300)\n",
    "        self.fc2 = nn.Linear(300, 128)\n",
    "        self.fc3 = nn.Linear(128, 31)\n",
    "\n",
    "        self.train_accuracy = Accuracy(task='multiclass', num_classes=31)\n",
    "        self.train_f1 = F1Score(task='multiclass', num_classes=31)\n",
    "        self.train_precisn = Precision(task='multiclass', num_classes=31)\n",
    "        self.train_rocAUC = AUROC(task='multiclass', num_classes=31)\n",
    "\n",
    "        self.val_accuracy = Accuracy(task='multiclass', num_classes=31)\n",
    "        self.val_f1 = F1Score(task='multiclass', num_classes=31)\n",
    "        self.val_precisn = Precision(task='multiclass', num_classes=31)\n",
    "        self.val_rocAUC = AUROC(task='multiclass', num_classes=31)\n",
    "\n",
    "        self.test_accuracy = Accuracy(task='multiclass', num_classes=31)\n",
    "        self.test_f1 = F1Score(task='multiclass', num_classes=31)\n",
    "        self.test_precisn = Precision(task='multiclass', num_classes=31)\n",
    "        self.test_rocAUC = AUROC(task='multiclass', num_classes=31)\n",
    "        self.confusion_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=31)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # block 1\n",
    "        x = x.unsqueeze(dim=1)\n",
    "        x = self.conv1D_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x= self.maxPool1D_1(x)\n",
    "\n",
    "\n",
    "        # block 2\n",
    "        x = self.conv1D_2(x)\n",
    "        x = torch.relu(x)\n",
    "        x= self.maxPool1D_2(x)\n",
    "\n",
    "        # block 3\n",
    "        x = self.conv1D_3(x)\n",
    "        x = torch.relu(x)\n",
    "        x= self.maxPool1D_3(x)\n",
    "\n",
    "        # block 4\n",
    "        x = self.conv1D_4(x)\n",
    "        x = torch.relu(x)\n",
    "        x= self.maxPool1D_4(x)\n",
    "\n",
    "        # Flatten\n",
    "\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.unsqueeze(dim=1)\n",
    "\n",
    "        y_pred = self.forward(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_pred,y.long())\n",
    "        self.log('train_loss', loss, on_epoch=True, on_step=True,prog_bar=True)\n",
    "        self.log('train_acc_step', self.train_accuracy(y_pred, y.long(), ), on_epoch=True, on_step=True,prog_bar=False)\n",
    "        self.log('train_f1_step', self.train_f1(y_pred, y.long(), ), on_epoch=True, on_step=True,prog_bar=False)\n",
    "        self.log('train_precision_step', self.train_precisn(y_pred, y.long(), ), on_epoch=True, on_step=True,)\n",
    "        self.log('train_auc_step', self.train_rocAUC(y_pred, y.long(), ), on_epoch=True, on_step=True,prog_bar=False )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.unsqueeze(dim=1)\n",
    "        y_pred = self.forward(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_pred,y.long())\n",
    "        self.log('validation_loss', loss)\n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "        self.log('val_acc', self.val_accuracy(y_pred, y.long(), ),on_epoch=True, prog_bar=True)\n",
    "        self.log('val_f1', self.val_f1(y_pred, y.long(), ),on_epoch=True, prog_bar=True)\n",
    "        self.log('val_precision', self.val_precisn(y_pred, y.long(), ),on_epoch=True, prog_bar=True)\n",
    "        self.log('val_auc', self.val_rocAUC(y_pred, y.long(), ),on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        x, y = batch\n",
    "        x = x.unsqueeze(dim=1)\n",
    "        # img = x.view(-1,3,IMG_SIZE,IMG_SIZE)\n",
    "        y_pred = self.forward(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_pred,y.long())\n",
    "        # self.log('test_loss', loss)\n",
    "        # self.log('test_loss', loss, on_epoch=True)\n",
    "        self.log('test_acc', self.test_accuracy(y_pred, y.long(), ),)\n",
    "        self.log('test_f1', self.test_f1(y_pred, y.long(), ),)\n",
    "        self.log('test_precision', self.test_precisn(y_pred, y.long(), ),)\n",
    "        self.log('test_auc', self.test_rocAUC(y_pred, y.long(), ),)\n",
    "        self.confusion_matrix(y_pred, y.long())\n",
    "        print(self.confusion_matrix)\n",
    "        # self.logger.experiment.add_confusion_matrix('confusion_matrix', self.confusion_matrix)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "model = CNN1D(input_shape=4097,)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T16:13:50.527093Z",
     "start_time": "2023-06-06T16:13:50.520376Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [2, 14, 15]\n",
      "-- >> Start of training phase << --\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'task_labels'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 55\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCurrent Classes: \u001B[39m\u001B[38;5;124m\"\u001B[39m, experience\u001B[38;5;241m.\u001B[39mclasses_in_this_experience)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m# train returns a dictionary which contains all the metric values\u001B[39;00m\n\u001B[0;32m---> 55\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mcl_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexperience\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining completed\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mComputing accuracy on the whole test set\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Pytorch39/lib/python3.9/site-packages/avalanche/training/templates/base_sgd.py:146\u001B[0m, in \u001B[0;36mBaseSGDTemplate.train\u001B[0;34m(self, experiences, eval_streams, **kwargs)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    140\u001B[0m           experiences: Union[CLExperience,\n\u001B[1;32m    141\u001B[0m                              ExpSequence],\n\u001B[1;32m    142\u001B[0m           eval_streams: Optional[Sequence[Union[CLExperience,\n\u001B[1;32m    143\u001B[0m                                                 ExpSequence]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    144\u001B[0m           \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 146\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexperiences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_streams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluator\u001B[38;5;241m.\u001B[39mget_last_metrics()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Pytorch39/lib/python3.9/site-packages/avalanche/training/templates/base.py:116\u001B[0m, in \u001B[0;36mBaseTemplate.train\u001B[0;34m(self, experiences, eval_streams, **kwargs)\u001B[0m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperience \u001B[38;5;129;01min\u001B[39;00m experiences:\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_before_training_exp(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_exp\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexperience\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_streams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_training_exp(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_training(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Pytorch39/lib/python3.9/site-packages/avalanche/training/templates/base_sgd.py:264\u001B[0m, in \u001B[0;36mBaseSGDTemplate._train_exp\u001B[0;34m(self, experience, eval_streams, **kwargs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stop_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m--> 264\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_training_epoch(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Pytorch39/lib/python3.9/site-packages/avalanche/training/templates/update_type/sgd_update.py:21\u001B[0m, in \u001B[0;36mSGDUpdate.training_epoch\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Forward\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_before_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmb_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Loss & Backward\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Pytorch39/lib/python3.9/site-packages/avalanche/training/templates/problem_type/supervised_problem.py:27\u001B[0m, in \u001B[0;36mSupervisedProblem.forward\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;124;03m\"\"\"Compute the model's output given the current mini-batch.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mavalanche_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmb_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmb_task_id\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Pytorch39/lib/python3.9/site-packages/avalanche/models/utils.py:13\u001B[0m, in \u001B[0;36mavalanche_forward\u001B[0;34m(model, x, task_labels)\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model(x, task_labels)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# no task labels\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Pytorch39/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "\u001B[0;31mTypeError\u001B[0m: forward() missing 1 required positional argument: 'task_labels'"
     ]
    }
   ],
   "source": [
    "from avalanche.models import MTSimpleMLP, MTSimpleCNN\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.training import Naive, EWC\n",
    "from avalanche.evaluation.metrics import accuracy_metrics, loss_metrics, timing_metrics, forgetting_metrics, \\\n",
    "    cpu_usage_metrics, disk_usage_metrics\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.logging import TensorboardLogger, TextLogger, InteractiveLogger\n",
    "import avalanche as avl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "benchmark = prep_benchmark(train_loc='./DATA/TRAIN_DATA.csv', test_loc='./DATA/TEST_DATA.csv')\n",
    "\n",
    "\n",
    "# log to Tensorboard\n",
    "tb_logger = TensorboardLogger()\n",
    "\n",
    "# log to text file\n",
    "text_logger = TextLogger(open('log.txt', 'a'))\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    # confusion_matrix_metrics(num_classes=benchmark['inc_bench'].n_classes, save_image=False,\n",
    "    #                          stream=True),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger, tb_logger]\n",
    ")\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE (EWC)\n",
    "# model2 = avl.models.PNN(in_features=4096,hidden_features_per_column=1000,num_layers=7,)\n",
    "# model3 =  MTSimpleCNN()\n",
    "model4 = MTSimple1DCNN()\n",
    "# TODO: 4096 is too much i might need to pass thought the cnn first and then pass it to the pnn\n",
    "cl_strategy = EWC(\n",
    "    model4, torch.optim.Adam(model.parameters(), lr=0.001,),\n",
    "    CrossEntropyLoss(), train_mb_size=500, train_epochs=100, eval_mb_size=100,\n",
    "    evaluator=eval_plugin,ewc_lambda=0.4)\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in benchmark.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(benchmark.test_stream))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T16:13:52.035484Z",
     "start_time": "2023-06-06T16:13:50.532282Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "res = cl_strategy.eval(benchmark.test_stream)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d = eval_plugin.get_last_metrics()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model4\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
